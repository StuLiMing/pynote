{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from public import *\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 tensor 的创建和常用属性\n",
    "\n",
    "## 1.1 `torch.tensor()`\n",
    "创建一个 tensor 的基本方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 `torch.arange()`\n",
    "与 `np.arange()` 类似\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 用特定初始值创建\n",
    "+ `torch.ones(shape)`\n",
    "+ `torch.zeros(shape)`\n",
    "+ `torch.ones_like(input)`：创建与 input tensor 的 shape 一样的全 1 的 tensor\n",
    "+ `torch.zeros_like(input)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones=torch.ones((4,3))\n",
    "zeros=torch.zeros((3,4))\n",
    "ones,zeros,torch.ones_like(zeros),torch.zeros_like(ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 `torch.randn()`\n",
    "采样自标准正态分布\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4636,  1.3151, -0.3694,  0.9357],\n",
       "        [ 0.1548, -0.5280,  0.0315, -0.4905],\n",
       "        [ 0.0761, -0.4114, -0.4731,  0.9872]])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 常用属性\n",
    "### 1.5.1 `tensor.shape`\n",
    "返回 tensor 的shape（类型是 `torch.Size`）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3]), torch.Size)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones.shape,type(ones.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 `tensor.dtype`\n",
    "元素的数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 `torch.numel()`\n",
    "返回 tensor 的元素的个数(注意是一个方法不是一个属性)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 从其他分布中采样\n",
    "### 1.7.1 正态分布\n",
    "`torch.normal(mean,std,size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1778,  0.6447, -1.8836, -1.4660],\n",
       "        [ 0.6768,  0.8468, -0.6561, -1.6748],\n",
       "        [-0.0041, -0.4722,  0.1850,  0.0489]])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(0,1,(3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 重新设置 tensor 的值\n",
    "### 1.6.1 `tensor.normal_()`\n",
    "用法类似于 `torch.normal()`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7422, -2.0430,  0.9462,  1.2674],\n",
       "        [-0.9434,  0.8481, -1.0347, -3.1147],\n",
       "        [ 1.6793, -1.2741, -0.9053,  0.0920]])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.ones((3,4))\n",
    "a.normal_(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 `tensor.fill_()`\n",
    "`tensor.fill_(value)`：用 value 填充 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.ones((3,4))\n",
    "a.fill_(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3 `tensor.zero_()`\n",
    "用 0 填充 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.ones((3,4))\n",
    "a.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 数学运算\n",
    "## 2.1 逐元素运算\n",
    "### 2.1.1 (逐元素)基本运算\n",
    "`+`, `-`, `*`, `/`, `**`, `//`, `%`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5, 7, 9]),\n",
       " tensor([-3, -3, -3]),\n",
       " tensor([ 4, 10, 18]),\n",
       " tensor([0.2500, 0.4000, 0.5000]),\n",
       " tensor([  1,  32, 729]),\n",
       " tensor([0, 0, 0]),\n",
       " tensor([1, 2, 3]))"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b=example(2)\n",
    "a+b,a-b,a*b,a/b,a**b,a//b,a%b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也允许 tensor 和一个常数的基本运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 4, 5]),\n",
       " tensor([-1,  0,  1]),\n",
       " tensor([2, 4, 6]),\n",
       " tensor([0.5000, 1.0000, 1.5000]),\n",
       " tensor([1, 4, 9]),\n",
       " tensor([0, 1, 1]),\n",
       " tensor([1, 0, 1]))"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+2,a-2,a*2,a/2,a**2,a//2,a%2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 (逐元素)一元运算\n",
    "+ `.log`：计算自然对数\n",
    "  除此之外还有 `.log2`，`.log10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 1.0000]), tensor([0.0000, 1.4427]), tensor([0.0000, 0.4343]))"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([1,torch.e])\n",
    "a.log(),a.log2(),a.log10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `.exp`：计算 e 的幂,除此之外还有 `.exp2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.7183,  7.3891, 20.0855]), tensor([2., 4., 8.]))"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3])\n",
    "a.exp(),a.exp2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 (逐元素)比较运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ True, False, False]),\n",
       " tensor([False,  True, False]),\n",
       " tensor([False, False,  True]),\n",
       " tensor([ True,  True, False]),\n",
       " tensor([ True, False,  True]))"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3]);b=torch.tensor([1,-1,4])\n",
    "a==b,a>b,a<b,a>=b,a<=b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 统计类运算\n",
    "### 2.2.1 (统计类)基本运算\n",
    "以下运算产生的结果都是一个单元素 tensor。\n",
    "+ `tensor.sum()`：求和\n",
    "+ `tensor.max()`：最大值\n",
    "+ `tensor.min()`：最小值\n",
    "+ `tensor.mean()`：均值，这里如果 tensor 默认为 int 类型，那么必须指定 `dtype`。\n",
    "+ `tensor.median()`：中位数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6), tensor(3), tensor(1), tensor(2.), tensor(2))"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3])\n",
    "a.sum(),a.max(),a.min(),a.mean(dtype=torch.float32),a.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 将 tensor 视作矩阵\n",
    "### 2.3.1 求转置\n",
    "`tensor.T`：注意这是一个属性不是一个方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]),\n",
       " tensor([[ 0,  4,  8],\n",
       "         [ 1,  5,  9],\n",
       "         [ 2,  6, 10],\n",
       "         [ 3,  7, 11]]))"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=example(1)\n",
    "a,a.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 点乘\n",
    "`torch.dot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor(32))"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b=example(2)\n",
    "a,b,torch.dot(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 矩阵乘法\n",
    "`torch.mm()` 和 `torch.matmul()` 都是计算矩阵乘法的方法，在处理二维 tensor 时，二者的行为一致，在处理高于二维的 tensor 时，前者会报错，而后者会执行批量矩阵乘法操作(并且也能执行广播机制)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5],\n",
       "         [6, 7, 8]]),\n",
       " tensor([[1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1]]),\n",
       " tensor([[ 3,  3,  3],\n",
       "         [12, 12, 12],\n",
       "         [21, 21, 21]]),\n",
       " tensor([[ 3,  3,  3],\n",
       "         [12, 12, 12],\n",
       "         [21, 21, 21]]))"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.arange(9,dtype=torch.int64).reshape(3,3)\n",
    "Y1=torch.ones((3,3),dtype=torch.int64)\n",
    "X,Y1,torch.mm(X,Y1),torch.matmul(X,Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3,  3,  3],\n",
       "         [12, 12, 12],\n",
       "         [21, 21, 21]],\n",
       "\n",
       "        [[ 3,  3,  3],\n",
       "         [12, 12, 12],\n",
       "         [21, 21, 21]],\n",
       "\n",
       "        [[ 3,  3,  3],\n",
       "         [12, 12, 12],\n",
       "         [21, 21, 21]]])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y2=torch.ones((3,3,3),dtype=torch.int64)\n",
    "torch.matmul(X,Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 连接、索引、变形等运算\n",
    "## 3.1 `torch.reshape()`\n",
    "与 `np.reshape()` 类似\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(12).reshape(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 `torch.cat()`\n",
    "`torch.cat(tensors,dim)`\n",
    "**参数**\n",
    "+ `tensors`：待连接的 tensors，这里必须被包装为一个元组\n",
    "+ `dim`：指定的连接的维度\n",
    "\n",
    "将多个 tensor 连接，`dim` 是指定的维度。\n",
    "如果 a.shape=(x1,y1), b.shape=(x2,y2),c=torch.cat((a,b),dim=0) 有 c.shape=(x1+x2,y);c=torch.cat((a,b),dim=1) 有 c.shape=(x,y1+y2)。这里另外一个维度必须对齐\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0,   1,   2,   3],\n",
       "         [  4,   5,   6,   7],\n",
       "         [  8,   9,  10,  11],\n",
       "         [-11, -10,  -9,  -8],\n",
       "         [ -7,  -6,  -5,  -4],\n",
       "         [ -3,  -2,  -1,   0]]),\n",
       " tensor([[  0,   1,   2,   3, -11, -10,  -9,  -8],\n",
       "         [  4,   5,   6,   7,  -7,  -6,  -5,  -4],\n",
       "         [  8,   9,  10,  11,  -3,  -2,  -1,   0]]))"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.arange(12).reshape(3,4);b=torch.arange(-11,1,1).reshape(3,4)\n",
    "torch.cat((a,b),dim=0),torch.cat((a,b),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 repeat\n",
    "\n",
    "重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
       "        [4, 5, 6, 4, 5, 6, 4, 5, 6],\n",
       "        [1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
       "        [4, 5, 6, 4, 5, 6, 4, 5, 6]])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([[1,2,3],[4,5,6]])\n",
    "a.repeat(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 gather\n",
    "`gather(dim,index)`\n",
    "\n",
    "\n",
    "一次索引出多个元素。\n",
    "+ `dim=0`，索引第 index[0] 行的第 0 列，第 index[1] 行的第 0 列，以此类推\n",
    "+ `dim=1`，索引第 0 行的第 index[0] 列，第 1 行的第 index[0] 列，以此类推"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5],\n",
       "         [9],\n",
       "         [1]]),\n",
       " tensor([[ 2],\n",
       "         [ 7],\n",
       "         [12]]))"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3, 4],\n",
    "                  [5, 6, 7, 8],\n",
    "                  [9, 10, 11, 12]])\n",
    "col_idx=torch.tensor([[1],\n",
    "                  [2],\n",
    "                  [3]])\n",
    "row_idx = torch.tensor([[1],\n",
    "                  [2],\n",
    "                  [0]])\n",
    "a.gather(dim=0, index=row_idx),a.gather(dim=1, index=col_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 机制类\n",
    "## 4.1 广播机制\n",
    "广播机制允许 shape 不匹配的 tensor 之间也能运算。这种机制的工作方式如下：\n",
    "\n",
    "1. 通过适当**复制**元素来扩展一个或两个数组，以便在转换之后，两个 tensor 具有相同的形状；\n",
    "\n",
    "2. 对生成的数组执行按元素操作。\n",
    "\n",
    "在大多数情况下，我们将沿着数组中长度为1的轴进行广播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a 和 b 相加，二者都会被广播为 3x2 的 tensor，分别为 tensor([[0,0],[1,1],[2,2]]) 和 tensor([[0,1],[0,1],[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播机制的存在使得某些“看起来不合法”的矩阵运算不会报错，这有时可能会导致意想不到的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [0, 1]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([1,2])\n",
    "b=torch.tensor([[1],[1]])\n",
    "print(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上例中 a 的唯一一行被复制，使之成为 2x2 的矩阵，b 的第一列都被复制，使之也成为 2x2 的矩阵。但是如果本来期望的是两个行向量相减或者两个 1x2 的矩阵相减，就会因为莫名其妙的结果而定位不到问题所在。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 明晰引用和内存的关系以避免不必要的内存分配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1855677563344, 1855532752560, False)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y=example(2)\n",
    "before=id(Y)\n",
    "Y=X+Y\n",
    "after=id(Y)\n",
    "before,after,before==after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现 Y 指向的内存变了，这里 `Y=X+Y` 开辟了一块新内存，用来存放 `X+Y`，然后将 Y 指向了这块新内存。开辟新内存不总是必要的，可能带来额外开销。以下两种方式可以避免开辟新内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1855677505872, 1855677505872, True)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y=example(2)\n",
    "before=id(Y)\n",
    "Y+=X\n",
    "after=id(Y)\n",
    "before,after,before==after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1855677563152, 1855677563152, True)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y=example(2)\n",
    "before=id(Y)\n",
    "Y[:]=X+Y\n",
    "after=id(Y)\n",
    "before,after,before==after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 与其他类型对象之间的转换\n",
    "## 5.1 tensor 和 np.array 之间的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), array([1, 2, 3], dtype=int64), torch.Tensor, numpy.ndarray)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,_=example(2)\n",
    "a_np=a.numpy()\n",
    "a,a_np,type(a),type(a_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二者共享底层内存，改变一个另一个也会改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1, -2,  3]), array([-1, -2,  3], dtype=int64))"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]=-1;a_np[1]=-2\n",
    "a,a_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 np.array 或普通的列表转为 tensor 不存在这种内存共享。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-1, 2, 3], tensor([ 1, -2,  3]))"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_lst=[1,2,3]\n",
    "a_tensor=torch.tensor(a_lst)\n",
    "a_lst[0]=-1;a_tensor[1]=-2\n",
    "a_lst,a_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  2,  3]), tensor([ 1., -2.,  3.]))"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_np=np.array([1,2,3])\n",
    "a_tensor=torch.Tensor(a_np)\n",
    "a_np[0]=-1;a_tensor[1]=-2\n",
    "a_np,a_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 将 tensor 转为标量\n",
    "要将大小为1的tensor转换为Python标量，我们可以调用 item 函数或 Python 的内置函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([3.5])\n",
    "a,a.item(),float(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 GPU 相关\n",
    "+ 判断 GPU 是否可用\n",
    "    ```python\n",
    "    torch.cuda.is_available()\n",
    "    ```\n",
    "\n",
    "+ 设置设备\n",
    "  ```python\n",
    "  # gpu\n",
    "  torch.device(\"cuda\") \n",
    "  # cpu\n",
    "  torch.device(\"cpu\")\n",
    "  ```\n",
    "\n",
    "+ 常用的语句\n",
    "  ```python\n",
    "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "  ```\n",
    "\n",
    "+ 将 nn.Module 类型的模型加载到指定设备\n",
    "  ```python\n",
    "  module.to(device)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 自动微分相关\n",
    "## 6.1 `requires_grad=True`\n",
    "`requires_grad` 是创建 tensor 时可以指定的一个参数，默认为 `False` ，如果指定为 `True`，则代表这个 tensor 是模型的参数，需要计算梯度。这个参数会将同名属性 `requires_grad` 置为一样的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [1.],\n",
       "         [1.]], requires_grad=True),\n",
       " True)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.ones((3,1),requires_grad=True)\n",
    "a,a.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以通过方法 `tensor.requires_grad_(True)` 来改变属性`requires_grad` 的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.ones((3))\n",
    "a.requires_grad_(True)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.2 `torch.no_grad()`\n",
    "一般用法\n",
    "：\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    ... # other code\n",
    "```\n",
    "\n",
    "这是一个上下文管理器（context manager），用于临时禁用梯度计算。这在执行一些操作但又不希望这些操作对梯度进行跟踪时非常有用，比如在评估模型时或者在手动更新模型参数时。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 `tensor.grad` 和 `tensor.backward()`\n",
    "`tensor.grad` 是一个属性，存储的是梯度值(tensor)，默认为 `None`。只要 tensor 的 `requires_grad=True` 才有梯度。这个梯度存储在自变量上，那是对于哪个函数而言的呢，这取决于是在哪个函数上调用的 `backward()` 方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 `tensor.grad.zero_()`\n",
    "这个方法会将 tensor 的梯度清零。默认情况下，tensor 上的梯度是会累积的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 2., 4., 6.])\n",
      "tensor([1., 3., 5., 7.])\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4.0,requires_grad=True)\n",
    "y = torch.dot(x, x)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "z=x.sum()\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "x.grad.zero_()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 torch.nn.Moudle\n",
    "\n",
    "### 3.1 self.register_buffer\n",
    "\n",
    "用于注册一个张量作为模型的一部分，但这个张量不是模型参数（即不会在训练过程中更新）。使用`register_buffer` 注册的张量会自动被模型的 `state_dict` 包含，这意味着它们会随模型一起保存和加载，但它们不会被视为模型的参数，所以在优化过程中不会被更新。\n",
    "\n",
    "一般用于保存一些固定的常数。\n",
    "\n",
    "比如 `self.register_buffer(\"betas\",betas)`，那么可以通过 `self.betas` 来获取这些参数\n",
    "\n",
    "## 4 torch.nn.functional\n",
    "\n",
    "一般 `import torch.nn.functional as F`\n",
    "\n",
    "+ `l1_loss()`：L1 loss\n",
    "+ `mse_loss()`：L2 loss\n",
    "+  `kl_div(input,target)`：KL 散度。如果要计算 $KL(P||Q)$，应该 `kl_div(Q.log(),P)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn.Linear\n",
    "创建一个全连接层。\n",
    "**参数**\n",
    "`Linear(in_features,out_features,bias,device,dtype)`\n",
    "+ `in_features`：输入特征个数\n",
    "+ `out_features`：输出特征个数\n",
    "+ `bias`：是否带 bias，默认为 `True`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=1, bias=True)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear=nn.Linear(5,1)\n",
    "linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可通过函数 `parameters()` 获取所有参数（用来传给优化器）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001B00EFF1A80>"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以访问其属性 `weights` 和 `bias` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.2718, -0.1603, -0.0721, -0.0358,  0.2985]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0767], requires_grad=True))"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight,linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.date` 可以获得 `weights` 和 `bias` 对应的 `tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2718, -0.1603, -0.0721, -0.0358,  0.2985]]), tensor([0.0767]))"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight.data,linear.bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn.Sequential\n",
    "一个网络一般是一个Sequential类的实例。 Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(2, 2),nn.Linear(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过索引访问其每一个图层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " Linear(in_features=2, out_features=2, bias=True),\n",
       " Linear(in_features=2, out_features=1, bias=True))"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(net),net[0],net[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也能通过 `parameters()` 获取所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001B00EFF1C40>"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn.MSELoss\n",
    "`MSELoss()` 是 L2 loss，也叫均方误差(MSE loss)。默认情况下它返回所有样本损失的平均值。这里前面不带系数 1/2。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.optim.SGD\n",
    "小批量随机梯度下降的实现。\n",
    "**参数**\n",
    "`SGD(params,lr)`\n",
    "+ `params`：要优化的参数\n",
    "+ `lr`：学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**方法**\n",
    "+ `trainer.zero_grad()`：将参数的导数清0，类似于 `param.grad.zero_()`\n",
    "+ `trainer.step()`：执行一次梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.utils.data\n",
    "## `TensorDataset()` \n",
    "`TensorDataset()` 是一个封装张量数据的数据集类，用来多个张量组合成一个数据集，返回一个 `TensorDataset` 实例\n",
    "## `DataLoader()`\n",
    "返回一个迭代器，用于支持自动批处理等功能。\n",
    "**参数**\n",
    "`DataLoader(dataset,batch_size,suffle)`\n",
    "+ `dataset`：一个 `TensorDataset` 实例\n",
    "+ `batch_size`：batch size\n",
    "+ `shuffle`：指示是否在每个epoch开始时随机打乱数据集中的样本顺序。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
